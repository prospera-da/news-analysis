{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**DEMO - NEWS SCRAPING**"
   ],
   "metadata": {
    "id": "HUVKficcK7Re"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Install Libraries**"
   ],
   "metadata": {
    "id": "15rJP6TD8zK6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PEQgqeo67mVt",
    "outputId": "64d5d054-cf80-482d-fdbf-2bcc5354bd88"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
      "Requirement already satisfied: lxml[html_clean] in /usr/local/lib/python3.11/dist-packages (5.3.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.1.3)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: lxml-html-clean in /usr/local/lib/python3.11/dist-packages (from lxml[html_clean]) (0.4.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
      "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k lxml[html_clean]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Import Libraries / Packages**"
   ],
   "metadata": {
    "id": "tmWT8DcF88Ni"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ],
   "metadata": {
    "id": "i-G9sOGv9NeF"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Define Functions (if any)**"
   ],
   "metadata": {
    "id": "AB8SrQ8U_1Ju"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to scrape news articles from Detik\n",
    "def scrape_news_tag(tag_url):\n",
    "\n",
    "    response = requests.get(tag_url)\n",
    "    page = 1\n",
    "    news_data = []\n",
    "\n",
    "    while True:\n",
    "        url = f\"{tag_url}{page}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if page > 1 or response.status_code != 200:\n",
    "            print(\"Failed to access the main page or no more pages.\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        articles = soup.find_all(\"article\")\n",
    "\n",
    "        if not articles:\n",
    "            print(\"No more articles found, stopping crawl.\")\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            link_tag = article.find(\"a\", href=True)\n",
    "            if link_tag:\n",
    "                article_url = link_tag[\"href\"]\n",
    "                try:\n",
    "                    article_response = requests.get(article_url)\n",
    "                    if article_response.status_code == 200:\n",
    "                        article_soup = BeautifulSoup(\n",
    "                            article_response.content, \"html.parser\"\n",
    "                        )\n",
    "                        title = (\n",
    "                            article_soup.find(\"h1\").text\n",
    "                            if article_soup.find(\"h1\")\n",
    "                            else \"No Title\"\n",
    "                        )\n",
    "                        content = \" \".join([p.text for p in article_soup.find_all(\"p\")])\n",
    "\n",
    "                        # Extract and format published date from detail__date class\n",
    "                        date_div = article_soup.find(\"div\", class_=\"detail__date\")\n",
    "                        if date_div:\n",
    "                            raw_date = date_div.text.strip()\n",
    "                            # Handle date format variations by extracting date using regex\n",
    "                            date_match = re.search(\n",
    "                                r\"\\d{2} \\w+ \\d{4} \\d{2}:\\d{2}\", raw_date\n",
    "                            )\n",
    "                            if date_match:\n",
    "                                try:\n",
    "                                    published_date = pd.to_datetime(\n",
    "                                        date_match.group(),\n",
    "                                        format=\"%d %b %Y %H:%M\",\n",
    "                                        errors=\"coerce\",\n",
    "                                    ).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                                except Exception as e:\n",
    "                                    published_date = \"Unknown\"\n",
    "                            else:\n",
    "                                published_date = \"Unknown\"\n",
    "                        else:\n",
    "                            published_date = \"Unknown\"\n",
    "\n",
    "                        author = (\n",
    "                            article_soup.find(class_=\"detail__author\").text.strip()\n",
    "                            if article_soup.find(class_=\"detail__author\")\n",
    "                            else \"Unknown\"\n",
    "                        )\n",
    "                        news_data.append(\n",
    "                            {\n",
    "                                \"Title\": title,\n",
    "                                \"Content\": content,\n",
    "                                \"Published Date\": published_date,\n",
    "                                \"Author\": author,\n",
    "                            }\n",
    "                        )\n",
    "                    else:\n",
    "                        print(f\"Failed to access article: {article_url}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scraping article: {article_url}, Error: {e}\")\n",
    "\n",
    "        print(f\"Page {page} processed.\")\n",
    "        page += 1\n",
    "\n",
    "    # Convert to DataFrame and sort by published date descending\n",
    "    df = pd.DataFrame(news_data)\n",
    "    df[\"Published Date\"] = pd.to_datetime(df[\"Published Date\"], errors=\"coerce\")\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "id": "3wFdjCX0_1kI"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Sample - Scrape from a pre-defined article**"
   ],
   "metadata": {
    "id": "U72W4O9386ld"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "url = \"https://finance.detik.com/berita-ekonomi-bisnis/d-7774570/pengusaha-lirik-peluang-di-balik-ancaman-perang-dagang-trump\"\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "print(\"Title: \", article.title)\n",
    "print(\"Content: \", article.text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDYYEcf67rHL",
    "outputId": "ec40969a-0c07-4aa0-c545-1b795f7f7ff4"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Title:  Pengusaha Lirik Peluang di Balik Ancaman Perang Dagang Trump\n",
      "Content:  Presiden Amerika Serikat (AS) Donald Trump memulai perang dagang dengan menaikkan tarif impor di sejumlah negara, termasuk China. Soal ini, pihak pengusaha mengaku harus pintar-pintar dalam melihat peluang yang timbul dari perang dagang ini.\n",
      "\n",
      "Ketua Dewan Pertimbangan Kamar Dagang dan Industri (Kadin) Indonesia, Arsjad Rasjid, mengatakan ada dua sisi yang bisa dilihat dari hingar-bingar perang dagang Trump.\n",
      "\n",
      "\"Yang dilakukan adalah lebih baik untuk Amerika sendiri. Untuk kita, kita lihat peluangnya. Misalnya, kalau mereka tidak mau beli produk China, kalau bisa dari Indonesia, bagaimana?\" Ucap Arsjad saat ditemui di konferensi pers Indonesia Economic Summit, Jakarta, Rabu (12/2/2025).\n",
      "\n",
      "ADVERTISEMENT SCROLL TO CONTINUE WITH CONTENT\n",
      "\n",
      "Menurut Arsjad, dengan begitu, ada potensi ke depan bahwa pengusaha China akan lebih banyak investasi di Indonesia. Hal ini juga ditujukan supaya usaha tetap berjalan.\n",
      "\n",
      "\"Karena kalau tidak, tidak bisa di jualkan. Take the positive side. Sebenarnya, dalam sisi apa yang dilakukan Trump, banyak hal yang bisa di-leverage oleh Indonesia,\" kata Arsjad menambahkan.\n",
      "\n",
      "ADVERTISEMENT\n",
      "\n",
      "Bahkan, Arsjad bilang, penting buat Indonesia atau bahkan ASEAN dapat membicarakan soal perannya bagi AS bisa jadi rantai pasok ke depannya.\n",
      "\n",
      "\"Penting juga untuk Indonesia bicara sama Trump. Bicara mengenai, eh bagaimana nih? Kenapa? Kenapa tidak Indonesia? Kenapa tidak ASEAN? Indonesia dan ASEAN menjadi apa? The supply chain,\" tandasnya.\n",
      "\n",
      "Sebagai informasi tambahan, Donald Trump, mulai memerintahkan kenaikan tarif 25% untuk impor produk-produk Kanada dan Meksiko serta China 10%. Ketetapan ini dinilai akan memicu perang dagang baru yang memperlambat laju pertumbuhan ekonomi global.\n",
      "\n",
      "Simak juga Video 'Program Efisiensi Trump Dikritik, Elon Musk: Rakyat Telah Memilih':\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Sample - Scrape news articles related to 'perang dagang' from Detik.**"
   ],
   "metadata": {
    "id": "gqsHLHTKKaY-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"Scrape news articles related to 'perang dagang' from Detik.\"\"\"\n",
    "\n",
    "tag_url = \"https://www.detik.com/tag/perang-dagang/?sortby=time&page=\"\n",
    "\n",
    "news_df = scrape_news_tag(tag_url)\n",
    "news_df.sort_values(by=\"Published Date\", ascending=False, inplace=True)\n",
    "print(news_df.head())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GA53HMh_Fah",
    "outputId": "b89c8533-5c34-4e9c-fc0b-2f66d00a23d2"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Page 1 processed.\n",
      "Failed to access the main page or no more pages.\n",
      "                                               Title  \\\n",
      "0  \\r\\n        Pengusaha Lirik Peluang di Balik A...   \n",
      "1  \\r\\n        IHSG Rontok hingga ke Level 6.500-...   \n",
      "2  \\r\\n        Perang Dagang AS Vs China Bisa Unt...   \n",
      "3  \\r\\n        Bisnis di Perbatasan AS Bisa Kocar...   \n",
      "4  \\r\\n        Trump Bakal Umumkan Tarif Impor Ba...   \n",
      "\n",
      "                                             Content      Published Date  \\\n",
      "0  Presiden Amerika Serikat (AS) Donald Trump mem... 2025-02-12 12:23:00   \n",
      "1  Indeks Harga Saham Gabungan (IHSG) kembali ter... 2025-02-10 14:20:00   \n",
      "2  Presiden Amerika Serikat (AS) Donald Trump mem... 2025-02-10 13:40:00   \n",
      "3  Ketidakpastian seputar usulan tarif dan kebija... 2025-02-08 20:45:00   \n",
      "4  Presiden Amerika Serikat (AS) Donald Trump ber... 2025-02-08 19:15:00   \n",
      "\n",
      "                                Author  \n",
      "0     Amanda Christabel - detikFinance  \n",
      "1     Amanda Christabel - detikFinance  \n",
      "2       Retno Ayuningru - detikFinance  \n",
      "3     Amanda Christabel - detikFinance  \n",
      "4  Shafira Cendra Arini - detikFinance  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Save to Excel (if necessary)**"
   ],
   "metadata": {
    "id": "jO-pFp4fG3ZC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if news_df is not None:\n",
    "    news_df.to_excel(\"perang_dagang_news_detik.xlsx\", index=False)\n",
    "    print(\"News articles saved to perang_dagang_news_detik.xlsx\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7X5AwJn_e1S",
    "outputId": "ca06fa09-9b5d-4a2c-bbfc-5864fad508e1"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "News articles saved to perang_dagang_news_detik.xlsx\n"
     ]
    }
   ]
  }
 ]
}